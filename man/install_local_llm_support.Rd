% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/zzz.R
\name{install_local_llm_support}
\alias{install_local_llm_support}
\title{Install Local LLM Support}
\usage{
install_local_llm_support()
}
\value{
Invisible \code{TRUE} on success.
}
\description{
Installs llama-cpp-python for running local GGUF models with
\code{\link{local_AIGENIE}}. On Apple Silicon Macs, this includes
Metal acceleration support for fast inference.
}
\details{
After installation, you can use any GGUF model file with \code{local_AIGENIE()}.
Download GGUF models from HuggingFace (search for "GGUF" format).

Popular model recommendations:
\itemize{
\item \strong{Llama 3 8B}: Good balance of quality and speed
\item \strong{Mistral 7B}: Fast with good quality
\item \strong{Qwen 2.5}: Strong multilingual support
}
}
\examples{
\dontrun{
# Install local LLM support
install_local_llm_support()

# Then download a GGUF model and use with local_AIGENIE
results <- local_AIGENIE(
  item.attributes = my_traits,
  model.path = "~/models/llama-3-8b.Q4_K_M.gguf",
  embedding.model = "bert-base-uncased"
)
}

}
\seealso{
\code{\link{local_AIGENIE}},
\code{\link{reinstall_python_env}}.
}
