% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/item_generation.R
\name{generate_items_via_local_llm}
\alias{generate_items_via_local_llm}
\title{Generate Items Using Local LLM (GGUF)}
\usage{
generate_items_via_local_llm(
  main.prompts,
  system.role,
  model.path,
  temperature,
  top.p,
  adaptive,
  silently,
  target.N,
  n.ctx = 4096,
  n.gpu.layers = -1,
  max.tokens = 1024
)
}
\arguments{
\item{main.prompts}{Named list of prompts}

\item{system.role}{Character string with system role}

\item{model.path}{Path to local GGUF model file}

\item{temperature}{Numeric. Sampling temperature}

\item{top.p}{Numeric. Nucleus sampling parameter}

\item{adaptive}{Logical. Use adaptive generation?}

\item{silently}{Logical. Suppress messages?}

\item{target.N}{Named list of target counts}

\item{n.ctx}{Integer. Context window size}

\item{n.gpu.layers}{Integer. GPU layers (-1 for all)}

\item{max.tokens}{Integer. Max tokens per generation}
}
\value{
A list with 'items' data frame and 'successful' flag
}
\description{
Generates items using a locally installed GGUF model via llama-cpp-python.
}
\keyword{internal}
